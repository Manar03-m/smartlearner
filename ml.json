{
  "meta": {
    "topic": "machine_learning_basics",
    "track": "intro_to_ai",
    "version": "6.0",
    "created_at": "2025-11-29 19:30:00.000000Z"
  },
  "theory": [
    {
      "id": "ml_intro_en",
      "lang": "en",
      "title": "What is Machine Learning?",
      "content": "Machine Learning (ML) is a subset of AI where computers learn from data without being explicitly programmed for specific rules.\n\nMain Types:\n1. Supervised Learning: Learning with labeled data (e.g., predicting house prices based on size).\n2. Unsupervised Learning: Finding patterns in unlabeled data (e.g., grouping customers by purchasing behavior).\n3. Reinforcement Learning: Learning through trial and error (rewards/penalties)."
    },
    {
      "id": "gradient_descent_theory_en",
      "lang": "en",
      "title": "Optimization: Gradient Descent",
      "content": "Gradient Descent is an optimization algorithm used to minimize the cost function (error) of a model. Imagine walking down a mountain blindfolded: you take small steps in the steepest direction downhill until you reach the bottom (the minimum error)."
    },
    {
      "id": "linear_regression_theory_en",
      "lang": "en",
      "title": "Supervised Learning: Linear Regression",
      "content": "Linear Regression is a fundamental algorithm used to predict a continuous value (like price or temperature). It tries to find the 'line of best fit' that minimizes the error between predicted and actual values."
    },
    {
      "id": "regularization_theory_en",
      "lang": "en",
      "title": "Regularization (L1 & L2)",
      "content": "Regularization is a technique to prevent overfitting by penalizing complex models.\n1. L1 (Lasso): Adds the absolute value of coefficients as a penalty. It can shrink weights to zero, effectively performing feature selection.\n2. L2 (Ridge): Adds the squared magnitude of coefficients. It keeps all features but shrinks their influence."
    },
    {
      "id": "logistic_regression_theory_en",
      "lang": "en",
      "title": "Supervised Learning: Logistic Regression",
      "content": "Unlike Linear Regression, Logistic Regression is used for Classification (predicting categories like Yes/No, Spam/Not Spam). It uses the Sigmoid function to squash the output between 0 and 1, representing a probability."
    },
    {
      "id": "knn_theory_en",
      "lang": "en",
      "title": "Supervised Learning: K-Nearest Neighbors (KNN)",
      "content": "KNN is a simple, instance-based learning algorithm. To classify a new data point, it looks at the 'K' closest labeled data points and votes for the most common class. It's often called a 'lazy learner' because it doesn't learn a discriminative function from the training data but memorizes the training dataset instead."
    },
    {
      "id": "naive_bayes_theory_en",
      "lang": "en",
      "title": "Supervised Learning: Naive Bayes",
      "content": "Naive Bayes is a probabilistic classifier based on Bayes' Theorem. It is 'naive' because it assumes that all features are independent of each other (e.g., the presence of the word 'free' in an email is unrelated to the word 'money'), which simplifies calculation but works surprisingly well for text classification."
    },
    {
      "id": "svm_theory_en",
      "lang": "en",
      "title": "Supervised Learning: Support Vector Machines (SVM)",
      "content": "SVM is a powerful classifier that finds the best boundary (hyperplane) that separates two classes with the maximum margin (distance between the boundary and the nearest data points). It works well for high-dimensional spaces."
    },
    {
      "id": "decision_tree_theory_en",
      "lang": "en",
      "title": "Supervised Learning: Decision Trees",
      "content": "A Decision Tree is a flowchart-like structure where an internal node represents a feature (or attribute), the branch represents a decision rule, and each leaf node represents the outcome (class label). It splits data into smaller subsets based on questions like 'Is Age > 30?' to maximize information gain."
    },
    {
      "id": "random_forest_theory_en",
      "lang": "en",
      "title": "Ensemble Learning: Random Forest",
      "content": "Random Forest is an ensemble method that builds multiple Decision Trees during training and merges them to get a more accurate and stable prediction. It prevents the common problem of overfitting seen in single Decision Trees."
    },
    {
      "id": "boosting_theory_en",
      "lang": "en",
      "title": "Ensemble Learning: Gradient Boosting",
      "content": "Gradient Boosting builds models sequentially. Each new model (usually a tree) tries to correct the errors made by the previous models. Popular implementations include XGBoost, LightGBM, and CatBoost."
    },
    {
      "id": "kmeans_theory_en",
      "lang": "en",
      "title": "Unsupervised Learning: K-Means Clustering",
      "content": "K-Means is an algorithm that partitions data into 'K' distinct clusters. It works by initializing K centroids, assigning each data point to the nearest centroid, and then recalculating the centroids based on the mean of the points in the cluster. This process repeats until the centroids stop moving."
    },
    {
      "id": "pca_theory_en",
      "lang": "en",
      "title": "Unsupervised Learning: PCA (Dimensionality Reduction)",
      "content": "Principal Component Analysis (PCA) reduces the number of features in a dataset while keeping the most important information (variance). It transforms large datasets into smaller ones that are easier to explore and visualize, often used before training a model to speed it up."
    },
    {
      "id": "cross_validation_theory_en",
      "lang": "en",
      "title": "Evaluation: Cross-Validation (K-Fold)",
      "content": "Instead of a single Train/Test split, Cross-Validation splits the data into 'K' folds. The model is trained K times, each time using K-1 folds for training and 1 fold for testing. The final score is the average. This provides a more robust estimate of model performance."
    },
    {
      "id": "neural_net_intro_en",
      "lang": "en",
      "title": "Introduction to Neural Networks",
      "content": "A Neural Network mimics the human brain using layers of artificial neurons (nodes). The simplest form is a 'Perceptron'. Data flows from the Input Layer, through Hidden Layers (where weights are adjusted), to the Output Layer. It is the foundation of Deep Learning."
    },
    {
      "id": "activation_functions_theory_en",
      "lang": "en",
      "title": "Deep Learning: Activation Functions",
      "content": "Activation functions determine if a neuron should fire. Common types:\n1. ReLU (Rectified Linear Unit): f(x) = max(0, x). Most common in hidden layers.\n2. Sigmoid: S-shaped curve between 0 and 1. Used in output layers for binary classification.\n3. Softmax: Converts raw scores into probabilities. Used in output layers for multi-class classification."
    },
    {
      "id": "evaluation_metrics_theory_en",
      "lang": "en",
      "title": "Model Evaluation Metrics",
      "content": "Accuracy isn't always enough, especially with imbalanced data.\n\n1. Precision: Out of all positive predictions, how many were actually positive? (TP / (TP + FP))\n2. Recall (Sensitivity): Out of all actual positives, how many did we find? (TP / (TP + FN))\n3. F1-Score: The harmonic mean of Precision and Recall, useful when you need a balance between the two."
    }
  ],
  "examples": [
    {
      "id": "ex_linear_regression_sklearn",
      "lang": "en",
      "title": "Linear Regression with Scikit-Learn",
      "code": "import numpy as np\nfrom sklearn.linear_model import LinearRegression\n\n# 1. Prepare Data (Features X and Labels y)\n# X: House size (sq meters), y: Price (in $1000s)\nX = np.array([[50], [60], [80], [100], [120]])\ny = np.array([150, 180, 250, 310, 380])\n\n# 2. Initialize and Train Model\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# 3. Make a Prediction\nnew_house_size = np.array([[90]])\npredicted_price = model.predict(new_house_size)\n\nprint(f\"Predicted price for 90sqm: ${predicted_price[0]:.2f}k\")",
      "explanation": "We use the fit() method to train the model on known data (X, y) and predict() to estimate the price for a new house size."
    },
    {
      "id": "ex_logistic_regression_sklearn",
      "lang": "en",
      "title": "Logistic Regression (Binary Classification)",
      "code": "from sklearn.linear_model import LogisticRegression\nimport numpy as np\n\n# Data: [Hours Studied], Label: 0 (Fail) or 1 (Pass)\nX = np.array([[1], [2], [3], [5], [6], [7]])\ny = np.array([0, 0, 0, 1, 1, 1])\n\nmodel = LogisticRegression()\nmodel.fit(X, y)\n\n# Predict for 4 hours\nprediction = model.predict([[4]])\nprobability = model.predict_proba([[4]])\n\nprint(f\"Prediction (4 hrs): {'Pass' if prediction[0]==1 else 'Fail'}\")\nprint(f\"Probability of Passing: {probability[0][1]:.2f}\")",
      "explanation": "Logistic Regression predicts the probability of a class. Here, we predict if a student passes based on hours studied."
    },
    {
      "id": "ex_svm_classifier_sklearn",
      "lang": "en",
      "title": "Support Vector Machine (SVM)",
      "code": "from sklearn import svm\n\n# Data: [Height, Weight], Label: 0 (Cat), 1 (Dog)\nX = [[25, 4], [20, 3], [50, 20], [45, 18]]\ny = [0, 0, 1, 1]\n\nclf = svm.SVC(kernel='linear')\nclf.fit(X, y)\n\nprint(\"Prediction for [48, 19]:\", clf.predict([[48, 19]]))",
      "explanation": "SVM finds the optimal hyperplane to separate cats from dogs based on weight and height."
    },
    {
      "id": "ex_naive_bayes_sklearn",
      "lang": "en",
      "title": "Naive Bayes Classifier",
      "code": "from sklearn.naive_bayes import GaussianNB\nimport numpy as np\n\n# Features: [Height (cm), Weight (kg), Shoe Size]\n# Labels: 0 (Female), 1 (Male)\nX = np.array([[160, 60, 36], [175, 75, 42], [155, 50, 35], [180, 85, 44]])\ny = np.array([0, 1, 0, 1])\n\nclf = GaussianNB()\nclf.fit(X, y)\n\n# Predict gender for [170, 70, 40]\nprint(\"Predicted Gender:\", clf.predict([[170, 70, 40]]))",
      "explanation": "Gaussian Naive Bayes is suitable for continuous data features. It calculates the probability of each class."
    },
    {
      "id": "ex_train_test_split",
      "lang": "en",
      "title": "Data Splitting (Train/Test)",
      "code": "from sklearn.model_selection import train_test_split\n\n# Fake dataset\nX, y = np.arange(10).reshape(-1, 1), np.arange(10)\n\n# Split: 80% for training, 20% for testing\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(\"Training Set Size:\", len(X_train))\nprint(\"Testing Set Size:\", len(X_test))",
      "explanation": "Splitting data is crucial to evaluate how well the model performs on unseen data, preventing overfitting."
    },
    {
      "id": "ex_kfold_cv_sklearn",
      "lang": "en",
      "title": "K-Fold Cross-Validation",
      "code": "from sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\n\n# Fake dataset\nX, y = np.random.rand(100, 5), np.random.randint(0, 2, 100)\n\nmodel = LogisticRegression()\n\n# 5-Fold Cross Validation\nscores = cross_val_score(model, X, y, cv=5)\n\nprint(\"Scores for each fold:\", scores)\nprint(\"Average Accuracy:\", scores.mean())",
      "explanation": "We train the model 5 times on different splits of the data to get a realistic performance estimate."
    },
    {
      "id": "ex_grid_search_sklearn",
      "lang": "en",
      "title": "Hyperparameter Tuning with GridSearch",
      "code": "from sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\n\nX, y = [[1],[2],[3],[4],[10],[11]], [0,0,0,0,1,1]\n\n# Define parameter grid\nparam_grid = {'n_neighbors': [1, 3, 5]}\n\nknn = KNeighborsClassifier()\ngrid_search = GridSearchCV(knn, param_grid, cv=2)\ngrid_search.fit(X, y)\n\nprint(\"Best Parameters:\", grid_search.best_params_)",
      "explanation": "GridSearch automatically tries every combination of parameters (like K=1, K=3) to find the one that gives the best accuracy."
    },
    {
      "id": "ex_knn_classifier_en",
      "lang": "en",
      "title": "Classification using KNN",
      "code": "from sklearn.neighbors import KNeighborsClassifier\n\n# Data: [Age, Salary], Label: 0 (No Purchase), 1 (Purchase)\nX = [[20, 2000], [25, 2500], [35, 5000], [40, 6000]]\ny = [0, 0, 1, 1]\n\n# K=3 means we look at the 3 nearest neighbors\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X, y)\n\nnew_customer = [[30, 4000]]\nprint(\"Predicted Class:\", knn.predict(new_customer))",
      "explanation": "The algorithm calculates the distance between the new customer and previous customers to determine the most appropriate class."
    },
    {
      "id": "ex_random_forest_sklearn",
      "lang": "en",
      "title": "Random Forest Classifier",
      "code": "from sklearn.ensemble import RandomForestClassifier\n\n# Features: [Experience, Education Level] -> Hire?\nX = [[1, 1], [1, 5], [10, 1], [10, 5]]\ny = [0, 0, 1, 1]\n\nclf = RandomForestClassifier(n_estimators=10)\nclf.fit(X, y)\n\nprint(\"Hire candidate with 8 years exp?\", clf.predict([[8, 3]]))",
      "explanation": "We use 10 different trees (n_estimators=10) to vote on whether to hire the candidate."
    },
    {
      "id": "ex_gradient_boosting_sklearn",
      "lang": "en",
      "title": "Gradient Boosting Classifier",
      "code": "from sklearn.ensemble import GradientBoostingClassifier\n\n# X: features, y: labels\nX, y = [[1, 2], [3, 4], [5, 6], [7, 8]], [0, 0, 1, 1]\n\nclf = GradientBoostingClassifier(n_estimators=50, learning_rate=0.1)\nclf.fit(X, y)\n\nprint(\"Prediction:\", clf.predict([[6, 7]]))",
      "explanation": "Gradient Boosting trains trees one by one, where each new tree specifically fixes the mistakes of the previous ones."
    },
    {
      "id": "ex_kmeans_sklearn",
      "lang": "en",
      "title": "K-Means Clustering (Unsupervised)",
      "code": "from sklearn.cluster import KMeans\nimport numpy as np\n\n# Data points (2D coordinates)\nX = np.array([[1, 2], [1, 4], [1, 0], \n              [10, 2], [10, 4], [10, 0]])\n\n# We want 2 clusters\nkmeans = KMeans(n_clusters=2, random_state=0)\nkmeans.fit(X)\n\nprint(\"Cluster Labels:\", kmeans.labels_)\nprint(\"Cluster Centers:\", kmeans.cluster_centers_)",
      "explanation": "K-Means will likely group the first three points (close to x=1) as one cluster and the last three (close to x=10) as another."
    },
    {
      "id": "ex_pca_sklearn",
      "lang": "en",
      "title": "Dimensionality Reduction with PCA",
      "code": "from sklearn.decomposition import PCA\nimport numpy as np\n\n# 3D Data (X, Y, Z)\nX = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n\n# Reduce to 2D\npca = PCA(n_components=2)\nX_new = pca.fit_transform(X)\n\nprint(\"Original Shape:\", X.shape)\nprint(\"Reduced Shape:\", X_new.shape)",
      "explanation": "PCA reduced the 3-dimensional dataset to 2 dimensions, keeping the most significant variance."
    }
  ],
  "exercises": [
    {
      "id": "q1_identify_type_en",
      "lang": "en",
      "type": "conceptual",
      "level": "beginner",
      "question": "You want to group news articles into topics (Sports, Politics, Tech) but you don't have labeled data. Which type of ML should you use?",
      "options": ["Supervised Learning", "Unsupervised Learning", "Reinforcement Learning"],
      "solution_text": "Unsupervised Learning (specifically Clustering), because the data is unlabeled."
    },
    {
      "id": "q2_predict_logic_en",
      "lang": "en",
      "type": "coding",
      "level": "intermediate",
      "question": "Complete the code to calculate the Mean Squared Error (MSE) manually given two lists: actual values and predicted values.",
      "starter_code": "actual = [10, 20, 30]\npredicted = [12, 18, 33]\n# TODO: Calculate MSE\nmse = 0\n",
      "solution_code": "actual = [10, 20, 30]\npredicted = [12, 18, 33]\n\nerrors = []\nfor a, p in zip(actual, predicted):\n    errors.append((a - p) ** 2)\n\nmse = sum(errors) / len(errors)\nprint(\"MSE:\", mse)"
    },
    {
      "id": "q3_overfitting_en",
      "lang": "en",
      "type": "conceptual",
      "level": "intermediate",
      "question": "What do we mean by the term 'Overfitting'?",
      "solution_text": "Overfitting occurs when the model memorizes the training data and its noise instead of learning general patterns, causing it to fail when predicting on new data."
    },
    {
      "id": "q4_accuracy_calc_en",
      "lang": "en",
      "type": "coding",
      "level": "intermediate",
      "question": "Calculate accuracy given TP (True Positives), TN, FP, FN.",
      "starter_code": "TP = 50\nTN = 40\nFP = 5\nFN = 5\n# TODO: Calculate accuracy = (Correct Predictions) / (Total Predictions)\naccuracy = 0.0",
      "solution_code": "TP = 50\nTN = 40\nFP = 5\nFN = 5\ntotal = TP + TN + FP + FN\naccuracy = (TP + TN) / total\nprint(f\"Accuracy: {accuracy:.2f}\")"
    },
    {
      "id": "q5_bias_variance_en",
      "lang": "en",
      "type": "conceptual",
      "level": "advanced",
      "question": "High Bias usually leads to what kind of fitting problem?",
      "options": ["Overfitting", "Underfitting", "Perfect fitting"],
      "solution_text": "Underfitting. High bias means the model is too simple to capture the underlying pattern (e.g., using a straight line for a curved dataset)."
    },
    {
      "id": "q6_recall_calc_en",
      "lang": "en",
      "type": "coding",
      "level": "advanced",
      "question": "Calculate Recall manually. Recall = TP / (TP + FN).",
      "starter_code": "TP = 80\nFN = 20\n# TODO: Calculate recall\nrecall = 0.0",
      "solution_code": "TP = 80\nFN = 20\nrecall = TP / (TP + FN)\nprint(f\"Recall: {recall:.2f}\")"
    },
    {
      "id": "q7_f1_score_calc_en",
      "lang": "en",
      "type": "coding",
      "level": "advanced",
      "question": "Calculate the F1-Score given Precision and Recall.",
      "starter_code": "precision = 0.8\nrecall = 0.6\n# Formula: 2 * (P * R) / (P + R)\nf1 = 0.0",
      "solution_code": "precision = 0.8\nrecall = 0.6\nf1 = 2 * (precision * recall) / (precision + recall)\nprint(f\"F1 Score: {f1:.2f}\")"
    },
    {
      "id": "q8_gradient_descent_en",
      "lang": "en",
      "type": "conceptual",
      "level": "advanced",
      "question": "In Gradient Descent, what happens if the 'Learning Rate' is too high?",
      "solution_text": "If the learning rate is too high, the algorithm might take steps that are too large, overshooting the minimum error and failing to converge (it might bounce back and forth)."
    },
    {
      "id": "q9_naive_bayes_assumption",
      "lang": "en",
      "type": "conceptual",
      "level": "intermediate",
      "question": "Why is 'Naive Bayes' called 'Naive'?",
      "options": ["It is a simple algorithm", "It assumes features are independent", "It only works on small data"],
      "solution_text": "It assumes features are independent. It assumes the presence of a particular feature in a class is unrelated to the presence of any other feature."
    },
    {
      "id": "q10_pca_variance",
      "lang": "en",
      "type": "conceptual",
      "level": "advanced",
      "question": "Does PCA guarantee to keep ALL information from the original dataset?",
      "options": ["Yes", "No"],
      "solution_text": "No. PCA reduces dimensionality by discarding the components with the least variance (information). Some data loss is expected."
    },
    {
      "id": "q11_regularization_effect",
      "lang": "en",
      "type": "conceptual",
      "level": "advanced",
      "question": "Which Regularization technique can eliminate features (set weights to 0)?",
      "options": ["L1 (Lasso)", "L2 (Ridge)", "None"],
      "solution_text": "L1 (Lasso) can reduce coefficients to exactly zero, effectively performing feature selection."
    },
    {
      "id": "q12_activation_relu",
      "lang": "en",
      "type": "conceptual",
      "level": "intermediate",
      "question": "What is the output of the ReLU activation function for an input of -5?",
      "options": ["-5", "0", "5", "1"],
      "solution_text": "0. ReLU (Rectified Linear Unit) outputs x if x > 0, and 0 if x <= 0."
    }
  ]
}